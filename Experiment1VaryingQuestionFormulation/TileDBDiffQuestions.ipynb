{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51d5bf08-0ea9-43b9-b412-d25ae77d41f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain, ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers.txt import TextParser\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import Language, RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.tiledb import TileDB\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "os.environ[\"OPENAI_API_KEY\"] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc49d88c-9bae-4255-b3f5-d4672c9e0ee2",
   "metadata": {},
   "source": [
    "Initalizing LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97d80ceb-dfff-4549-9224-cdf798cca6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ym/q12h1czs7935v9b665jkynvh0000gn/T/ipykernel_89759/2032839490.py:1: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n",
      "/var/folders/ym/q12h1czs7935v9b665jkynvh0000gn/T/ipykernel_89759/2032839490.py:5: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  public_chatgpt = ConversationChain(llm=llm)\n",
      "/Users/sihamargaw/.local/pipx/.cache/5c9468f9a0a782a/lib/python3.12/site-packages/pydantic/main.py:212: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "/var/folders/ym/q12h1czs7935v9b665jkynvh0000gn/T/ipykernel_89759/2032839490.py:8: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(f\"AI: {public_chatgpt.run(question)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is langchain?\n",
      "AI: Langchain is a decentralized platform that uses blockchain technology to create a secure and transparent system for language learning. It allows users to connect with language tutors, access learning materials, and track their progress in real-time. The platform also uses smart contracts to ensure fair transactions between users and tutors.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0\n",
    ")\n",
    "public_chatgpt = ConversationChain(llm=llm)\n",
    "question = \"What is langchain?\"\n",
    "print(f\"User: {question}\")\n",
    "print(f\"AI: {public_chatgpt.run(question)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aa58e5-b6ef-4fb3-8d69-bdabce264672",
   "metadata": {},
   "source": [
    "Data Preprocessing, generating embedding, and indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7be5ea4-f3e1-46e9-b08b-02b99b7c4471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of raw documents loaded: 319\n",
      "Number of document chunks: 1358\n",
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ym/q12h1czs7935v9b665jkynvh0000gn/T/ipykernel_89759/269744480.py:31: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding = OpenAIEmbeddings()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated. Total time: 8.324710130691528s\n",
      "Indexing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sihamargaw/.local/pipx/.cache/5c9468f9a0a782a/lib/python3.12/site-packages/tiledb/cloud/config.py:96: UserWarning: You must first login before you can run commands. Please run tiledb.cloud.login.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing completed. Total time: 1.3249428272247314s\n",
      "Number of vector embeddings stored in TileDB-Vector-Search: 1358\n"
     ]
    }
   ],
   "source": [
    "# Parse markdown documents and split them into text chunks\n",
    "documentation_path = \"/Users/sihamargaw/Desktop/langchain/docs\"\n",
    "# this langchain loader will be used to load the .mdx files from the path\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    documentation_path,\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".mdx\"],\n",
    "    parser=TextParser()\n",
    ")\n",
    "# this langchain splitter will be used to split the documents into size 1000 text chunks\n",
    "splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.MARKDOWN,\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "# load the documents using the loader created\n",
    "documents = loader.load()\n",
    "print(f\"Number of raw documents loaded: {len(documents)}\")\n",
    "\n",
    "# split the documents using the splitter created\n",
    "documents = splitter.split_documents(documents)\n",
    "documents = [d for d in documents if len(d.page_content) > 5]\n",
    "texts = [d.page_content for d in  documents]\n",
    "metadatas = [d.metadata for d in documents]\n",
    "print(f\"Number of document chunks: {len(texts)}\")\n",
    "\n",
    "\n",
    "# Generate embeddings for each document chunk using OpenAI embedding model\n",
    "print(\"Generating embeddings...\")\n",
    "t1 = time.time()\n",
    "embedding = OpenAIEmbeddings()\n",
    "text_embeddings = embedding.embed_documents(texts)\n",
    "# store each text chunk with it's corresponding embedding as pairs\n",
    "text_embedding_pairs = list(zip(texts, text_embeddings))\n",
    "t2 = time.time()\n",
    "print(f\"Embeddings generated. Total time: {(t2-t1)}s\")\n",
    "\n",
    "\n",
    "# Index document chunks using a TileDB IVF_FLAT index, using langchain's from_embeddings method\n",
    "print(\"Indexing...\")\n",
    "# each pair will be stored locally indexed\n",
    "tiledb_index_uri = \"./tiledb_langchain_documentation_index\"\n",
    "if os.path.isdir(tiledb_index_uri):\n",
    "    shutil.rmtree(tiledb_index_uri)\n",
    "db = TileDB.from_embeddings(\n",
    "    text_embedding_pairs, \n",
    "    embedding, \n",
    "    index_uri=tiledb_index_uri,\n",
    "    index_type=\"IVF_FLAT\",\n",
    "    allow_dangerous_deserialization=True,  # Enabling the bypass for pickle deserialization\n",
    "    metadatas=metadatas)\n",
    "t3 = time.time()\n",
    "print(f\"Indexing completed. Total time: {(t3-t2)}s\")\n",
    "print(f\"Number of vector embeddings stored in TileDB-Vector-Search: {len(text_embeddings)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf491004-4c9b-4548-8b89-c1ddf3ca7f1d",
   "metadata": {},
   "source": [
    "Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d72e3f15-1ea2-49e2-87af-6453df497cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is langchain?\n",
      "AI: LangChain is a framework for developing applications powered by large language models (LLMs). It simplifies every stage of the LLM application lifecycle, including development, productionization, and deployment. LangChain provides building blocks, components, and third-party integrations to help developers build applications using LLMs effectively.\n",
      "\n",
      "Total Latency for Retrieval: 1.8465557098388672 seconds\n"
     ]
    }
   ],
   "source": [
    "# loads the tileDB indexes we stored in the previous code block and loads it with the same embedding model used\n",
    "db = TileDB.load(\n",
    "    index_uri=tiledb_index_uri, \n",
    "    embedding=embedding,\n",
    "    allow_dangerous_deserialization=True  # Enable deserialization despite the warning\n",
    ")\n",
    "# initalizes our LLM\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0\n",
    ")\n",
    "# using langchain framework\n",
    "# sets retriver as tileDB\n",
    "# does a similarity search between embeddings created for question/prompt and embeddings we have stored in TileDB, and returns top 5 most\n",
    "# similar embeddings(text chunks)\n",
    "# retriver automatically converts question to embeddings\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5},\n",
    ")\n",
    "# Use Langchain framework(ConversationalRetrievalChain a class in langchain) to make the backend for the LLM the retriver in our case it's tileDB\n",
    "private_chatgpt = ConversationalRetrievalChain.from_llm(llm, retriever=retriever)\n",
    "\n",
    "question = \"What is langchain?\"\n",
    "\n",
    "start_time = time.time() # Measure latency for retrieval\n",
    "response = private_chatgpt.run({'question': question, 'chat_history': ''})\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate TileDB’s latency when retrieving data.\n",
    "total_latency = end_time - start_time\n",
    "\n",
    "print(f\"User: {question}\")\n",
    "print(f\"AI: {response}\\n\")\n",
    "print(f\"Total Latency for Retrieval: {total_latency} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0519b96-001d-4f6d-9144-9325bb82b916",
   "metadata": {},
   "source": [
    "For different question formulations record time taken for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24427025-b8a0-4ecf-9b39-f2e516bf8d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing question: What is LangChain?\n",
      "Latency for 'What is LangChain?': 1.7185 seconds\n",
      "Processing question: Can you explain LangChain?\n",
      "Latency for 'Can you explain LangChain?': 2.6070 seconds\n",
      "Processing question: Describe the functionality of LangChain.\n",
      "Latency for 'Describe the functionality of LangChain.': 3.3793 seconds\n",
      "Processing question: What are the main features of LangChain?\n",
      "Latency for 'What are the main features of LangChain?': 3.5977 seconds\n",
      "Processing question: How does LangChain work?\n",
      "Latency for 'How does LangChain work?': 2.4425 seconds\n",
      "Processing question: Can you provide an overview of LangChain?\n",
      "Latency for 'Can you provide an overview of LangChain?': 2.4614 seconds\n"
     ]
    }
   ],
   "source": [
    "# List of different formulations of the same question\n",
    "import time\n",
    "questions = [\n",
    "    \"What is LangChain?\",\n",
    "    \"Can you explain LangChain?\",\n",
    "    \"Describe the functionality of LangChain.\",\n",
    "    \"What are the main features of LangChain?\",\n",
    "    \"How does LangChain work?\",\n",
    "    \"Can you provide an overview of LangChain?\",\n",
    "]\n",
    "\n",
    "# Dictionary to store latencies for each question\n",
    "latency_results_tileDB = {}\n",
    "\n",
    "# Iterate over each question, measure latency, and store the results\n",
    "for question in questions:\n",
    "    print(f\"Processing question: {question}\")\n",
    "    start_time = time.time()\n",
    "    response = private_chatgpt.run({'question': question, 'chat_history': ''})\n",
    "    end_time = time.time()\n",
    "    latency = end_time - start_time\n",
    "    latency_results_tileDB[question] = latency\n",
    "    print(f\"Latency for '{question}': {latency:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7189ec32-80ea-4027-9165-81a09a8067fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
